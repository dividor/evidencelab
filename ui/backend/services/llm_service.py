"""
LLM Service for generating AI summaries using LangChain
"""

import logging
import re
import sys
from pathlib import Path
from typing import Any, Dict, List

from deep_translator import GoogleTranslator
from jinja2 import Environment, FileSystemLoader
from langchain_core.messages import HumanMessage, SystemMessage

# Add utils to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from utils.langsmith_util import setup_langsmith_tracing  # noqa: E402
from utils.llm_factory import get_llm  # noqa: E402

# Setup LangSmith tracing
setup_langsmith_tracing()

logger = logging.getLogger(__name__)

# Initialize Jinja2 environment for prompt templates
PROMPTS_DIR = Path(__file__).resolve().parents[3] / "prompts"
jinja_env = Environment(loader=FileSystemLoader(str(PROMPTS_DIR)), autoescape=True)

# Load templates at module level
_system_template = jinja_env.get_template("ai_summary_system.j2")
_user_template = jinja_env.get_template("ai_summary_user.j2")


def render_prompt(
    query: str, results: List[Dict[str, Any]], max_results: int = 20
) -> str:
    """
    Render the full prompt for debugging/transparency purposes.

    Args:
        query: The search query string
        results: List of search results
        max_results: Maximum number of results to include

    Returns:
        The fully rendered prompt text with system and user messages
    """
    # Limit to top N results
    top_results = results[:max_results]

    # Render prompts from templates
    system_prompt = _system_template.render()
    user_prompt = _user_template.render(query=query, results=top_results)

    # Combine into full prompt
    full_prompt = f"SYSTEM MESSAGE:\n{system_prompt}\n\nUSER MESSAGE:\n{user_prompt}"

    return full_prompt


async def stream_ai_summary(
    query: str,
    results: List[Dict[str, Any]],
    max_results: int = 20,
    model_key: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
):
    """
    Stream an AI summary of search results token by token.

    Args:
        query: The search query string
        results: List of search results (dictionaries with title, organization, text, etc.)
        max_results: Maximum number of results to include in the prompt (default: 20)

    Yields:
        Individual tokens as they are generated by the LLM
    """
    try:
        # Limit to top N results
        top_results = results[:max_results]

        # Render prompts from templates
        system_prompt = _system_template.render()
        user_prompt = _user_template.render(query=query, results=top_results)

        logger.info(
            f"Streaming AI summary for query: '{query}' using {len(top_results)} results"
        )

        # Log the prompts
        logger.info("=" * 80)
        logger.info("AI Summary Request (Streaming)")
        logger.info("=" * 80)
        logger.info("SYSTEM PROMPT:")
        logger.info(system_prompt)
        logger.info("-" * 80)
        logger.info("USER PROMPT:")
        logger.info(user_prompt)
        logger.info("=" * 80)

        # Get LLM instance
        llm = get_llm(model=model_key, temperature=temperature, max_tokens=max_tokens)

        # Use LangChain's astream for async streaming
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ]

        # Stream tokens from the LLM
        accumulated = ""
        async for chunk in llm.astream(messages):
            if chunk.content:
                # Apply the same cleaning logic as the non-streaming version
                token = chunk.content
                accumulated += token
                yield token

        logger.info(f"✓ Streamed AI summary ({len(accumulated)} chars)")

    except Exception as e:
        logger.error(f"Error streaming AI summary: {e}", exc_info=True)
        raise


async def generate_ai_summary(
    query: str,
    results: List[Dict[str, Any]],
    max_results: int = 20,
    model_key: str | None = None,
    temperature: float | None = None,
    max_tokens: int | None = None,
) -> str:
    """
    Generate an AI summary of search results using LangChain + HuggingFace Inference API.

    Args:
        query: The search query string
        results: List of search results (dictionaries with title, organization, text, etc.)
        max_results: Maximum number of results to include in the prompt (default: 20)

    Returns:
        Generated summary text
    """
    try:
        # Limit to top N results
        top_results = results[:max_results]

        # Render prompts from templates
        system_prompt = _system_template.render()
        user_prompt = _user_template.render(query=query, results=top_results)

        logger.info(
            f"Generating AI summary for query: '{query}' using {len(top_results)} results"
        )

        # Log the prompts
        logger.info("=" * 80)
        logger.info("AI Summary Request")
        logger.info("=" * 80)
        logger.info("SYSTEM PROMPT:")
        logger.info(system_prompt)
        logger.info("-" * 80)
        logger.info("USER PROMPT:")
        logger.info(user_prompt)
        logger.info("=" * 80)

        # Get LLM instance
        llm = get_llm(model=model_key, temperature=temperature, max_tokens=max_tokens)

        # Use LangChain's ainvoke for async invocation with chat format
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ]
        response = await llm.ainvoke(messages)

        # Extract the generated text and clean up any quotes or formatting
        summary = response.content.strip()

        # Remove surrounding quotes if present
        if summary.startswith('"') and summary.endswith('"'):
            summary = summary[1:-1]
        elif summary.startswith("'") and summary.endswith("'"):
            summary = summary[1:-1]

        # Remove any "User:" or "Assistant:" prefixes that might have been added
        lines = summary.split("\n")
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            # Skip lines that are just "User:" or "Assistant:" patterns
            if line.startswith("User:") or line.startswith("Assistant:"):
                # Extract text after the colon if there is any
                parts = line.split(":", 1)
                if len(parts) > 1 and parts[1].strip():
                    cleaned_lines.append(parts[1].strip())
            elif line:  # Only add non-empty lines
                cleaned_lines.append(line)

        summary = " ".join(cleaned_lines)

        # Log the response
        logger.info("AI SUMMARY RESPONSE:")
        logger.info(summary)
        logger.info("=" * 80)

        logger.info(f"✓ Generated AI summary ({len(summary)} chars)")

        return summary

    except Exception as e:
        logger.error(f"Error generating AI summary: {e}", exc_info=True)
        raise


async def translate_text(text: str, target_language: str) -> str:
    """
    Translate text using deep-translator (Google Translate) instead of LLM.
    Uses regex to protect reference numbers like [64] from being mangled.
    """
    if not text:
        return ""

    # Map full language names or codes to deep-translator ISO codes.
    # Note: "zh" is not accepted by GoogleTranslator; use "zh-CN".
    lang_map = {
        "english": "en",
        "french": "fr",
        "spanish": "es",
        "arabic": "ar",
        "chinese": "zh-CN",
        "portuguese": "pt",
        "russian": "ru",
        "swahili": "sw",
        "hindi": "hi",
        "bengali": "bn",
        "german": "de",
        "greek": "el",
        "italian": "it",
        "lithuanian": "lt",
        "vietnamese": "vi",
        "dutch": "nl",
        "polish": "pl",
        "turkish": "tr",
        "japanese": "ja",
        "korean": "ko",
        "en": "en",
        "fr": "fr",
        "es": "es",
        "ar": "ar",
        "zh": "zh-CN",
        "pt": "pt",
        "ru": "ru",
        "sw": "sw",
        "hi": "hi",
        "bn": "bn",
        "de": "de",
        "el": "el",
        "it": "it",
        "lt": "lt",
        "vi": "vi",
        "nl": "nl",
        "pl": "pl",
        "tr": "tr",
        "ja": "ja",
        "ko": "ko",
    }

    target_lang_code = lang_map.get(target_language.lower(), "en")

    try:
        # 1. Protect references: [64] -> __REF_64__
        protected_text = text
        ref_pattern = r"\[(\d+)\]"
        refs = re.findall(ref_pattern, text)
        for ref_num in set(refs):
            protected_text = protected_text.replace(
                f"[{ref_num}]", f"__REF_{ref_num}__"
            )

        # 2. Protect newlines to prevent flattening
        # Replace \n\n with __PARA__ and \n with __BR__
        protected_text = protected_text.replace("\n\n", " __PARA__ ")
        protected_text = protected_text.replace("\n", " __BR__ ")

        # 3. Perform translation
        # deep-translator is synchronous, suitable for direct call here.
        translator = GoogleTranslator(source="auto", target=target_lang_code)
        translated_text = translator.translate(protected_text)

        # 4. Restore references: __REF_64__ -> [64]
        if translated_text:
            # Restore references
            restore_ref_pattern = r"__\s*REF\s*_\s*(\d+)\s*__"

            def replace_match(match):
                return f"[{match.group(1)}]"

            final_text = re.sub(restore_ref_pattern, replace_match, translated_text)

            # Restore newlines (handling potential extra spaces added by translator)
            # __PARA__ -> \n\n
            final_text = re.sub(r"\s*__\s*PARA\s*__\s*", "\n\n", final_text)
            # __BR__ -> \n
            final_text = re.sub(r"\s*__\s*BR\s*__\s*", "\n", final_text)

            return final_text.strip()

        return text

    except Exception as e:
        logger.error(f"Translation failed: {e}")
        return text
