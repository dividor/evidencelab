# Production Docker Compose for evidencelab.ai
# Uses Caddy for automatic HTTPS with Let's Encrypt
#
# Usage:
#   docker compose -f docker-compose.prod.yml up -d --build
#
# Prerequisites:
#   - Domain evidencelab.ai pointing to this server's IP
#   - Ports 80 and 443 open in firewall
#   - .env file with HUGGINGFACE_API_KEY
#
# Memory limits prevent pipeline OOM from crashing UI/API:
#   - Pipeline: NOT STARTED in production (disabled)
#   - API: 6GB (1 worker)
#   - Qdrant: 15GB (vector DB)
#   - UI/Caddy: minimal (static serving)

services:
  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    # Prod: 127.0.0.1 only — host access for admin/scripts; not on public interface
    ports:
      - "127.0.0.1:6333:6333"
    environment:
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY}
    volumes:
      - ${DB_DATA_MOUNT:-/mnt/data/db}/qdrant:/qdrant/storage:z
      - ${DB_DATA_MOUNT:-/mnt/data/db}/backups:/qdrant/snapshots:z
    restart: unless-stopped
    networks:
      - app-network
    deploy:
      resources:
        limits:
          memory: 15G
        reservations:
          memory: 2G

  # Postgres sidecar (pgvector)
  postgres:
    image: pgvector/pgvector:pg16
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-evidencelab}
      - POSTGRES_USER=${POSTGRES_USER:-evidencelab}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-evidencelab}
    volumes:
      - ${DB_DATA_MOUNT:-/mnt/data/db}/postgres:/var/lib/postgresql/data:z
    restart: unless-stopped
    networks:
      - app-network
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

  # Embedding Server (Infinity) - Shared GPU/CPU inference
  embedding-server:
    image: michaelf34/infinity:0.0.70
    container_name: embedding-server
    ports:
      - "7997:7997"
    environment:
      # Serve the same model as DENSE_EMBEDDING_MODEL in .env
      - INFINITY_MODEL_ID=${DENSE_EMBEDDING_MODEL:-intfloat/multilingual-e5-large}
      - INFINITY_BATCH_SIZE=32
      - PYTHONUNBUFFERED=1
    volumes:
      - ./logs:/logs
    entrypoint: []
    command: [ "/bin/sh", "-c", "infinity_emb v2 2>&1 | tee -a /logs/embedding_server.log" ]
    networks:
      - app-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G

  # Pipeline for data processing (ingestion / batch jobs)
  # DISABLED in production - not started by default
  # To enable, uncomment this section and restart with: docker compose -f docker-compose.prod.yml up -d pipeline
  # pipeline:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #     target: runtime
  #   image: evidencelab-ai-python:latest
  #   container_name: pipeline
  #   volumes:
  #     - .:/app
  #     - ${DATA_MOUNT_PATH:-./data}:/app/data
  #     - ${CACHE_MOUNT_PATH:-/mnt/data/cache}/huggingface:/root/.cache/huggingface
  #     - ${CACHE_MOUNT_PATH:-/mnt/data/cache}/fastembed:/root/.cache/fastembed
  #
  #   env_file:
  #     - .env
  #   environment:
  #     - QDRANT_HOST=http://qdrant:6333
  #     - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
  #     - EMBEDDING_API_URL=http://embedding-server:7997
  #   depends_on:
  #     - qdrant
  #     - embedding-server
  #   networks:
  #     - app-network
  #   # Idle by default; exec into it for batch jobs
  #   command: tail -f /dev/null
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 8G
  #       reservations:
  #         memory: 2G

  # FastAPI backend (RAG API)
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    image: evidencelab-ai-python:latest
    container_name: api
    # Prod: 127.0.0.1 only — host access for scripts (e.g. verify_search_performance); not public
    ports:
      - "127.0.0.1:8000:8000"
    volumes:
      - ${DATA_MOUNT_PATH:-./data}:/app/data
      - ${CACHE_MOUNT_PATH:-/mnt/data/cache}/huggingface:/root/.cache/huggingface
      - ${CACHE_MOUNT_PATH:-/mnt/data/cache}/fastembed:/root/.cache/fastembed

    env_file:
      - .env
    environment:
      - QDRANT_HOST=http://qdrant:6333
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - POSTGRES_HOST=${POSTGRES_HOST:-postgres}
      - POSTGRES_PORT=${POSTGRES_PORT:-5432}
      - POSTGRES_DB=${POSTGRES_DB:-evidencelab}
      - POSTGRES_USER=${POSTGRES_USER:-evidencelab}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-evidencelab}
      - DATA_MOUNT_PATH=/app/data
      - PYTHONPATH=/app
      - API_SECRET_KEY=${API_SECRET_KEY}
      - CORS_ALLOWED_ORIGINS=${CORS_ALLOWED_ORIGINS:-https://evidencelab.ai}
      - RATE_LIMIT_SEARCH=${RATE_LIMIT_SEARCH:-30/minute}
      - RATE_LIMIT_DEFAULT=${RATE_LIMIT_DEFAULT:-60/minute}
      - RATE_LIMIT_AI=${RATE_LIMIT_AI:-10/minute}
    depends_on:
      - qdrant
      - postgres
    networks:
      - app-network
    working_dir: /app
    # Multi-worker Uvicorn for production concurrency
    command: uvicorn ui.backend.main:app --host 0.0.0.0 --port 8000 --workers 1
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 2G

  # React frontend (production build with nginx)
  ui:
    build:
      context: .
      dockerfile: ui/frontend/Dockerfile.prod
      args:
        - REACT_APP_AI_SUMMARY_ON=${REACT_APP_AI_SUMMARY_ON:-false}
        - REACT_APP_SEARCH_SEMANTIC_HIGHLIGHTS=${REACT_APP_SEARCH_SEMANTIC_HIGHLIGHTS:-true}
        - REACT_APP_PDF_SEMANTIC_HIGHLIGHTS=${REACT_APP_PDF_SEMANTIC_HIGHLIGHTS:-true}
        - REACT_APP_SEMANTIC_HIGHLIGHT_THRESHOLD=${REACT_APP_SEMANTIC_HIGHLIGHT_THRESHOLD:-0.4}
        - REACT_APP_API_KEY=${REACT_APP_API_KEY}
        - REACT_APP_API_BASE_URL=${REACT_APP_API_BASE_URL:-/api}
        - REACT_APP_BASE_PATH=${REACT_APP_BASE_PATH:-}
    container_name: ui
    # Only visible inside Docker; Caddy reverse-proxies to this
    expose:
      - "80"
    environment:
      - API_SECRET_KEY=${API_SECRET_KEY}
      - RATE_LIMIT_SEARCH=${RATE_LIMIT_SEARCH:-30/minute}
      - RATE_LIMIT_DEFAULT=${RATE_LIMIT_DEFAULT:-60/minute}
      - RATE_LIMIT_AI=${RATE_LIMIT_AI:-10/minute}
    depends_on:
      - api
    networks:
      - app-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 128M

  # Caddy reverse proxy + automatic HTTPS (Let's Encrypt)
  reverse-proxy:
    image: caddy:latest
    container_name: caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    environment:
      # API key for external Qdrant access (Container Apps workers)
      - QDRANT_API_KEY=${QDRANT_API_KEY}
    volumes:
      # Caddy stores certs here (persist across restarts)
      - ./caddy_data:/data
      - ./caddy_config:/config
      # Caddyfile configuration
      - ./Caddyfile:/etc/caddy/Caddyfile
    networks:
      - app-network
    depends_on:
      - api
      - ui
      - qdrant
      - postgres
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 64M

networks:
  app-network:
    driver: bridge
